{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Fine-Tuning Workflow in PostgreSQL\n",
    "\n",
    "This notebook demonstrates how to fine-tune a GPT-2 model that lives *inside* PostgreSQL using the SQL helpers shipped with the `pg_llm` extension.\n",
    "It walks through the entire lifecycle: importing weights, ingesting tokenizer assets, preparing a dataset, running training, and sampling\n",
    "text from the tuned model. Each step is annotated so you can adapt it to your own data sources or infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites\n",
    "\n",
    "Before running the notebook:\n",
    "\n",
    "1. Build and install the extension following the instructions in the repository README.\n",
    "2. Start a PostgreSQL instance with the extension available and set the `PGDSN` environment variable to your connection string (for example `postgresql://postgres@localhost:5432/postgres`).\n",
    "3. Download a GPT-2 checkpoint and tokenizer files from Hugging Face (`vocab.json`, `merges.txt`).\n",
    "4. Place any fine-tuning text files you want to ingest into a directory such as `./corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import textwrap\n",
    "\n",
    "import psycopg\n",
    "\n",
    "DSN = os.environ.get(\"PGDSN\", \"postgresql://postgres@localhost:5432/postgres\")\n",
    "MODEL_NAME = \"gpt2-small\"\n",
    "WEIGHTS_ARCHIVE = pathlib.Path(\"/mnt/models/gpt2-small.npz\")\n",
    "TOKENIZER_VOCAB = pathlib.Path(\"./gpt2/vocab.json\")\n",
    "TOKENIZER_MERGES = pathlib.Path(\"./gpt2/merges.txt\")\n",
    "CORPUS_DIR = pathlib.Path(\"./corpus\")\n",
    "CORPUS_GLOB = str(CORPUS_DIR / \"*.txt\")\n",
    "\n",
    "conn = psycopg.connect(DSN, autocommit=True)\n",
    "\n",
    "def run_sql(sql, params=None):\n",
    "    \"\"\"Execute a SQL statement and pretty-print any returned rows.\"\"\"\n",
    "    params = params or ()\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(sql, params)\n",
    "        if cur.description:\n",
    "            columns = [c.name for c in cur.description]\n",
    "            rows = cur.fetchall()\n",
    "            print(\" | \\t\".join(columns))\n",
    "            for row in rows:\n",
    "                print(\" | \\t\".join(str(v) for v in row))\n",
    "\n",
    "run_sql(\"CREATE EXTENSION IF NOT EXISTS pg_llm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Pretrained Weights\n",
    "\n",
    "The helper function `pg_llm_import_npz` loads a converted GPT-2 checkpoint and splits it into the tensor layout expected by the extension.\n",
    "Use the `scripts/convert_gpt2_checkpoint.py` helper to generate the NPZ archive beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sql(\n",
    "    \"SELECT pg_llm_import_npz(%s, %s)\",\n",
    "    params=(str(WEIGHTS_ARCHIVE), MODEL_NAME),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ingest Tokenizer Assets\n",
    "\n",
    "Tokenizer merges and vocabulary entries live in dedicated tables. The `ingest_tokenizer.py` script loads them for you, but you can also use SQL\n",
    "if the files are already accessible from the database server. Here we call the helper script via a notebook shell cell for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/ingest_tokenizer.py \\\n",
    "    --dsn {DSN} \\\n",
    "    --model {MODEL_NAME} \\\n",
    "    --vocab {TOKENIZER_VOCAB} \\\n",
    "    --merges {TOKENIZER_MERGES} \\\n",
    "    --truncate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare a Tokenized Dataset\n",
    "\n",
    "The dataset loader converts raw UTF-8 text into `(tokens, target)` arrays that match the model's context window. Each row represents a single training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/prepare_dataset.py \\\n",
    "    --dsn {DSN} \\\n",
    "    --tokenizer {MODEL_NAME} \\\n",
    "    --input {CORPUS_GLOB} \\\n",
    "    --block-size 1024 \\\n",
    "    --batch-size 512 \\\n",
    "    --truncate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After ingestion you can inspect the dataset distribution directly from SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sql(\n",
    "    textwrap.dedent(\"\"\"\n",
    "    SELECT COUNT(*) AS examples, AVG(array_length(tokens, 1)) AS avg_tokens\n",
    "      FROM llm_dataset\n",
    "\"\"\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Launch Fine-Tuning\n",
    "\n",
    "`llm_train` runs forward and backward passes entirely in SQL. Configure the scheduler, optimizer, and dropout parameters to match your experiment.\n",
    "For a warm-up schedule you can reuse the same defaults that `llm_train_e2e.sql` demonstrates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sql(\n",
    "    textwrap.dedent(\"\"\"\n",
    "    SELECT llm_train(%s, 1000, 12, 12, 768, 50257,\n",
    "                   dropout_p => 0.1,\n",
    "                   beta1 => 0.9,\n",
    "                   beta2 => 0.999,\n",
    "                   eps => 1e-8,\n",
    "                   wd => 0.01,\n",
    "                   lr_max => 2.5e-4,\n",
    "                   warmup => 2000,\n",
    "                   grad_clip => 1.0)\n",
    "    \"\"\"),\n",
    "    params=(MODEL_NAME,),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Training Progress\n",
    "\n",
    "Training updates stream into `llm_train_log`. Query it to plot or log the loss curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sql(\n",
    "    textwrap.dedent(\"\"\"\n",
    "    SELECT step, loss\n",
    "      FROM llm_train_log\n",
    "     WHERE model = %s\n",
    "  ORDER BY step DESC\n",
    "     LIMIT 10\n",
    "    \"\"\"),\n",
    "    params=(MODEL_NAME,),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Text from the Tuned Model\n",
    "\n",
    "The sampling helpers work on the fine-tuned parameters immediately. Use `llm_sampling.sql` as a reference for nucleus or top-k sampling strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"PostgreSQL models can\"\n",
    "run_sql(\n",
    "    textwrap.dedent(\"\"\"\n",
    "    SELECT llm_generate(%s, 64, 0.8, 40, 0.95)\n",
    "    \"\"\"),\n",
    "    params=(prompt,),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clean Up (Optional)\n",
    "\n",
    "Drop temporary artifacts after experimentation to reclaim storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sql(\n",
    "    textwrap.dedent(\"\"\"\n",
    "    DELETE FROM llm_train_log WHERE model = %s;\n",
    "    DELETE FROM llm_dataset;\n",
    "    DELETE FROM llm_param WHERE model = %s;\n",
    "    \"\"\"),\n",
    "    params=(MODEL_NAME, MODEL_NAME),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "* If `pg_llm_import_npz` fails, ensure the path is readable by the PostgreSQL server process.\n",
    "* The helper scripts assume Hugging Face checkpoints; custom models need matching tensor names.\n",
    "* Use `EXPLAIN (ANALYZE, BUFFERS)` around training queries to profile performance bottlenecks.\n",
    "* Consider using [pgvector](https://github.com/pgvector/pgvector) or similar extensions if you want hybrid retrieval + generation workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}